"""
æµ‹è¯•å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
éªŒè¯å®ç°çš„æ­£ç¡®æ€§å’ŒåŠŸèƒ½
"""

import torch
import torch.nn as nn
import numpy as np
from multihead_attention import (
    ScaledDotProductAttention, 
    MultiHeadAttention,
    PositionalEncoding,
    create_padding_mask,
    create_look_ahead_mask
)


def test_scaled_dot_product_attention():
    """æµ‹è¯•ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›"""
    print("æµ‹è¯•ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›...")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size = 2
    n_heads = 3
    seq_len_q = 4
    seq_len_k = 5
    d_k = 8
    d_v = 8
    
    query = torch.randn(batch_size, n_heads, seq_len_q, d_k)
    key = torch.randn(batch_size, n_heads, seq_len_k, d_k)
    value = torch.randn(batch_size, n_heads, seq_len_k, d_v)
    
    # åˆ›å»ºæ³¨æ„åŠ›å±‚
    attention = ScaledDotProductAttention(dropout=0.0)
    
    # å‰å‘ä¼ æ’­
    output, weights = attention(query, key, value)
    
    # éªŒè¯å½¢çŠ¶
    assert output.shape == (batch_size, n_heads, seq_len_q, d_v), \
        f"è¾“å‡ºå½¢çŠ¶é”™è¯¯: {output.shape}, æœŸæœ›: {(batch_size, n_heads, seq_len_q, d_v)}"
    
    assert weights.shape == (batch_size, n_heads, seq_len_q, seq_len_k), \
        f"æƒé‡å½¢çŠ¶é”™è¯¯: {weights.shape}, æœŸæœ›: {(batch_size, n_heads, seq_len_q, seq_len_k)}"
    
    # éªŒè¯æ³¨æ„åŠ›æƒé‡å’Œä¸º1
    weights_sum = weights.sum(dim=-1)
    assert torch.allclose(weights_sum, torch.ones_like(weights_sum), rtol=1e-5), \
        "æ³¨æ„åŠ›æƒé‡å’Œä¸ä¸º1"
    
    print("  âœ“ ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›æµ‹è¯•é€šè¿‡")
    return True


def test_multihead_attention_shapes():
    """æµ‹è¯•å¤šå¤´æ³¨æ„åŠ›å½¢çŠ¶"""
    print("æµ‹è¯•å¤šå¤´æ³¨æ„åŠ›å½¢çŠ¶...")
    
    # æµ‹è¯•1: æ ‡å‡†è‡ªæ³¨æ„åŠ›
    d_model = 64
    n_heads = 8
    batch_size = 4
    seq_len = 10
    
    attention = MultiHeadAttention(d_model, n_heads)
    x = torch.randn(batch_size, seq_len, d_model)
    
    output, weights = attention(x, x, x)
    
    assert output.shape == (batch_size, seq_len, d_model), \
        f"è¾“å‡ºå½¢çŠ¶é”™è¯¯: {output.shape}"
    
    assert weights.shape == (batch_size, n_heads, seq_len, seq_len), \
        f"æƒé‡å½¢çŠ¶é”™è¯¯: {weights.shape}"
    
    print("  âœ“ æ ‡å‡†è‡ªæ³¨æ„åŠ›å½¢çŠ¶æµ‹è¯•é€šè¿‡")
    
    # æµ‹è¯•2: ä¸åŒé•¿åº¦çš„æŸ¥è¯¢å’Œé”®å€¼
    seq_len_q = 6
    seq_len_kv = 8
    
    query = torch.randn(batch_size, seq_len_q, d_model)
    key_value = torch.randn(batch_size, seq_len_kv, d_model)
    
    output, weights = attention(query, key_value, key_value)
    
    assert output.shape == (batch_size, seq_len_q, d_model), \
        f"äº¤å‰æ³¨æ„åŠ›è¾“å‡ºå½¢çŠ¶é”™è¯¯: {output.shape}"
    
    assert weights.shape == (batch_size, n_heads, seq_len_q, seq_len_kv), \
        f"äº¤å‰æ³¨æ„åŠ›æƒé‡å½¢çŠ¶é”™è¯¯: {weights.shape}"
    
    print("  âœ“ äº¤å‰æ³¨æ„åŠ›å½¢çŠ¶æµ‹è¯•é€šè¿‡")
    return True


def test_multihead_attention_masking():
    """æµ‹è¯•å¤šå¤´æ³¨æ„åŠ›æ©ç åŠŸèƒ½"""
    print("æµ‹è¯•å¤šå¤´æ³¨æ„åŠ›æ©ç åŠŸèƒ½...")
    
    d_model = 32
    n_heads = 4
    batch_size = 2
    seq_len = 5
    
    attention = MultiHeadAttention(d_model, n_heads, dropout=0.0)
    
    # åˆ›å»ºè¾“å…¥
    x = torch.randn(batch_size, seq_len, d_model)
    
    # åˆ›å»ºå¡«å……æ©ç 
    seq = torch.tensor([
        [1, 1, 1, 0, 0],  # å‰3ä¸ªæœ‰æ•ˆï¼Œå2ä¸ªå¡«å……
        [1, 1, 0, 0, 0]   # å‰2ä¸ªæœ‰æ•ˆï¼Œå3ä¸ªå¡«å……
    ])
    mask = create_padding_mask(seq)
    
    # åº”ç”¨å¸¦æ©ç çš„æ³¨æ„åŠ›
    output, weights = attention(x, x, x, mask=mask)
    
    # éªŒè¯å¡«å……ä½ç½®çš„æ³¨æ„åŠ›æƒé‡ä¸º0
    for i in range(batch_size):
        valid_len = (seq[i] != 0).sum().item()
        
        # æ£€æŸ¥å¡«å……ä½ç½®çš„æƒé‡
        for head_idx in range(n_heads):
            # æŸ¥è¯¢ä½ç½®åœ¨æœ‰æ•ˆèŒƒå›´å†…çš„å¡«å……é”®ä½ç½®
            for q in range(valid_len):
                for k in range(valid_len, seq_len):
                    assert weights[i, head_idx, q, k].abs() < 1e-6, \
                        f"å¡«å……ä½ç½®åº”æœ‰0æƒé‡ï¼Œä½†å¾—åˆ°: {weights[i, head_idx, q, k]}"
    
    print("  âœ“ å¡«å……æ©ç æµ‹è¯•é€šè¿‡")
    
    # æµ‹è¯•å‰ç»æ©ç 
    look_ahead_mask = create_look_ahead_mask(seq_len)
    output2, weights2 = attention(x, x, x, mask=look_ahead_mask)
    
    # éªŒè¯æœªæ¥ä½ç½®çš„æƒé‡ä¸º0
    for i in range(batch_size):
        for head_idx in range(n_heads):
            for q in range(seq_len):
                for k in range(q + 1, seq_len):  # æœªæ¥ä½ç½®
                    assert weights2[i, head_idx, q, k].abs() < 1e-6, \
                        f"æœªæ¥ä½ç½®åº”æœ‰0æƒé‡ï¼Œä½†å¾—åˆ°: {weights2[i, head_idx, q, k]}"
    
    print("  âœ“ å‰ç»æ©ç æµ‹è¯•é€šè¿‡")
    return True


def test_positional_encoding():
    """æµ‹è¯•ä½ç½®ç¼–ç """
    print("æµ‹è¯•ä½ç½®ç¼–ç ...")
    
    d_model = 16
    max_len = 20
    batch_size = 3
    seq_len = 10
    
    pos_enc = PositionalEncoding(d_model, max_len)
    
    # åˆ›å»ºè¾“å…¥
    x = torch.randn(batch_size, seq_len, d_model)
    
    # åº”ç”¨ä½ç½®ç¼–ç 
    x_with_pe = pos_enc(x)
    
    # éªŒè¯å½¢çŠ¶
    assert x_with_pe.shape == x.shape, \
        f"ä½ç½®ç¼–ç åå½¢çŠ¶æ”¹å˜: {x_with_pe.shape} != {x.shape}"
    
    # éªŒè¯ä¸åŒä½ç½®æœ‰ä¸åŒçš„ç¼–ç 
    pos_encoding_values = pos_enc.pe[0, :seq_len, :]
    
    # æ£€æŸ¥ä¸åŒä½ç½®æ˜¯å¦ä¸åŒ
    for i in range(seq_len - 1):
        for j in range(i + 1, seq_len):
            assert not torch.allclose(pos_encoding_values[i], pos_encoding_values[j]), \
                f"ä½ç½®{i}å’Œä½ç½®{j}çš„ç¼–ç ç›¸åŒ"
    
    # éªŒè¯å‘¨æœŸæ€§
    # ä½ç½®ç¼–ç åº”è¯¥å…·æœ‰å‘¨æœŸæ€§æ¨¡å¼
    period = 10000  # æ ¹æ®å…¬å¼ï¼Œå‘¨æœŸä¸10000ç›¸å…³
    
    print("  âœ“ ä½ç½®ç¼–ç æµ‹è¯•é€šè¿‡")
    return True


def test_gradient_flow():
    """æµ‹è¯•æ¢¯åº¦æµåŠ¨"""
    print("æµ‹è¯•æ¢¯åº¦æµåŠ¨...")
    
    d_model = 48
    n_heads = 6
    batch_size = 2
    seq_len = 7
    
    attention = MultiHeadAttention(d_model, n_heads)
    
    # åˆ›å»ºéœ€è¦æ¢¯åº¦çš„è¾“å…¥
    x = torch.randn(batch_size, seq_len, d_model, requires_grad=True)
    
    # å‰å‘ä¼ æ’­
    output, _ = attention(x, x, x)
    
    # åˆ›å»ºè™šæ‹ŸæŸå¤±
    target = torch.randn_like(output)
    loss = nn.MSELoss()(output, target)
    
    # åå‘ä¼ æ’­
    loss.backward()
    
    # éªŒè¯æ¢¯åº¦å­˜åœ¨
    assert x.grad is not None, "è¾“å…¥æ¢¯åº¦ä¸ºNone"
    assert not torch.all(x.grad == 0), "è¾“å…¥æ¢¯åº¦å…¨ä¸º0"
    
    # éªŒè¯æ‰€æœ‰å‚æ•°éƒ½æœ‰æ¢¯åº¦
    for name, param in attention.named_parameters():
        assert param.grad is not None, f"å‚æ•°{name}çš„æ¢¯åº¦ä¸ºNone"
        assert not torch.all(param.grad == 0), f"å‚æ•°{name}çš„æ¢¯åº¦å…¨ä¸º0"
    
    print("  âœ“ æ¢¯åº¦æµåŠ¨æµ‹è¯•é€šè¿‡")
    return True


def test_attention_properties():
    """æµ‹è¯•æ³¨æ„åŠ›æ€§è´¨"""
    print("æµ‹è¯•æ³¨æ„åŠ›æ€§è´¨...")
    
    d_model = 24
    n_heads = 3
    batch_size = 1
    seq_len = 4
    
    attention = MultiHeadAttention(d_model, n_heads, dropout=0.0)
    
    # æµ‹è¯•1: ç›¸åŒè¾“å…¥äº§ç”Ÿç›¸åŒè¾“å‡ºï¼ˆç¡®å®šæ€§ï¼‰
    x = torch.randn(batch_size, seq_len, d_model)
    
    # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ä»¥ç¡®ä¿ç¡®å®šæ€§
    attention.eval()
    
    with torch.no_grad():
        output1, _ = attention(x, x, x)
        output2, _ = attention(x, x, x)
    
    assert torch.allclose(output1, output2, rtol=1e-6), \
        "ç›¸åŒè¾“å…¥äº§ç”Ÿä¸åŒè¾“å‡º"
    
    print("  âœ“ ç¡®å®šæ€§æµ‹è¯•é€šè¿‡")
    
    # æµ‹è¯•2: çº¿æ€§æ€§è´¨ï¼ˆè¿‘ä¼¼ï¼‰
    x1 = torch.randn(batch_size, seq_len, d_model)
    x2 = torch.randn(batch_size, seq_len, d_model)
    alpha = 0.3
    beta = 0.7
    
    attention.train()  # è®­ç»ƒæ¨¡å¼
    output_linear = attention(alpha * x1 + beta * x2, 
                              alpha * x1 + beta * x2,
                              alpha * x1 + beta * x2)[0]
    
    output_separate = alpha * attention(x1, x1, x1)[0] + \
                      beta * attention(x2, x2, x2)[0]
    
    # æ³¨æ„ï¼šç”±äºsoftmaxçš„éçº¿æ€§ï¼Œè¿™ä¸æ˜¯ç²¾ç¡®çš„çº¿æ€§å…³ç³»
    # æˆ‘ä»¬åªæ£€æŸ¥å®ƒä»¬ä¸æ˜¯å®Œå…¨ä¸åŒçš„
    diff = (output_linear - output_separate).abs().mean()
    assert diff.item() < 1.0, f"çº¿æ€§æ€§è´¨å·®å¼‚å¤ªå¤§: {diff}"
    
    print("  âœ“ è¿‘ä¼¼çº¿æ€§æ€§è´¨æµ‹è¯•é€šè¿‡")
    return True


def test_performance():
    """æµ‹è¯•æ€§èƒ½"""
    print("æµ‹è¯•æ€§èƒ½...")
    
    import time
    
    d_model = 512
    n_heads = 8
    batch_size = 16
    seq_len = 64
    
    attention = MultiHeadAttention(d_model, n_heads)
    x = torch.randn(batch_size, seq_len, d_model)
    
    # é¢„çƒ­
    for _ in range(5):
        _ = attention(x, x, x)
    
    # æ€§èƒ½æµ‹è¯•
    num_iterations = 100
    start_time = time.time()
    
    for _ in range(num_iterations):
        output, _ = attention(x, x, x)
    
    end_time = time.time()
    avg_time = (end_time - start_time) / num_iterations
    
    print(f"  âš¡ å¹³å‡æ¨ç†æ—¶é—´: {avg_time*1000:.2f} ms")
    print(f"  âš¡ FPS: {1/avg_time:.1f}")
    
    # éªŒè¯åœ¨åˆç†æ—¶é—´å†…å®Œæˆ
    assert avg_time < 0.1, f"æ¨ç†æ—¶é—´å¤ªé•¿: {avg_time}"
    
    print("  âœ“ æ€§èƒ½æµ‹è¯•é€šè¿‡")
    return True


def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("=" * 60)
    print("å¼€å§‹æµ‹è¯•å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶")
    print("=" * 60)
    
    tests = [
        ("ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›", test_scaled_dot_product_attention),
        ("å¤šå¤´æ³¨æ„åŠ›å½¢çŠ¶", test_multihead_attention_shapes),
        ("æ³¨æ„åŠ›æ©ç ", test_multihead_attention_masking),
        ("ä½ç½®ç¼–ç ", test_positional_encoding),
        ("æ¢¯åº¦æµåŠ¨", test_gradient_flow),
        ("æ³¨æ„åŠ›æ€§è´¨", test_attention_properties),
        ("æ€§èƒ½", test_performance),
    ]
    
    passed = 0
    failed = 0
    
    for test_name, test_func in tests:
        try:
            print(f"\n{test_name}:")
            if test_func():
                passed += 1
        except Exception as e:
            print(f"  âœ— æµ‹è¯•å¤±è´¥: {e}")
            failed += 1
    
    print("\n" + "=" * 60)
    print("æµ‹è¯•ç»“æœæ€»ç»“:")
    print(f"  é€šè¿‡: {passed}")
    print(f"  å¤±è´¥: {failed}")
    print(f"  æ€»è®¡: {passed + failed}")
    
    if failed == 0:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
    else:
        print(f"\nâš ï¸  {failed}ä¸ªæµ‹è¯•å¤±è´¥")
    
    print("=" * 60)
    
    return failed == 0


if __name__ == "__main__":
    # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§
    torch.manual_seed(42)
    np.random.seed(42)
    
    success = run_all_tests()
    
    if success:
        print("\nå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶å®ç°æ­£ç¡®ï¼")
        print("\nå®ç°åŠŸèƒ½åŒ…æ‹¬:")
        print("1. âœ… ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›æ ¸å¿ƒè®¡ç®—")
        print("2. âœ… å¤šå¤´åˆ†å‰²ä¸åˆå¹¶")
        print("3. âœ… å¡«å……æ©ç å’Œå‰ç»æ©ç ")
        print("4. âœ… ä½ç½®ç¼–ç ")
        print("5. âœ… æ­£ç¡®çš„æ¢¯åº¦æµåŠ¨")
        print("6. âœ… åˆç†çš„æ€§èƒ½")
        
        print("\nå¯ä»¥ç”¨äº:")
        print("- Transformeræ¨¡å‹")
        print("- è‡ªæ³¨æ„åŠ›ç¼–ç å™¨")
        print("- äº¤å‰æ³¨æ„åŠ›è§£ç å™¨")
        print("- ä»»ä½•éœ€è¦åºåˆ—å»ºæ¨¡çš„ä»»åŠ¡")
    else:
        print("\nå®ç°å­˜åœ¨é—®é¢˜ï¼Œè¯·æ£€æŸ¥ä»£ç ï¼")
    
    exit(0 if success else 1)