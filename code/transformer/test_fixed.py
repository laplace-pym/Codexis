import os
import sys

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

# æµ‹è¯•å¯¼å…¥
try:
    from transformer import Transformer, MultiHeadAttention, PositionalEncoding
    print("âœ… æˆåŠŸå¯¼å…¥Transformeræ¨¡å—")
    
    # åˆ›å»ºä¸€ä¸ªå°å‹æ¨¡å‹æµ‹è¯•
    model = Transformer(
        src_vocab_size=100,
        tgt_vocab_size=100,
        d_model=64,
        n_layers=2,  # æ³¨æ„ï¼šå‚æ•°åæ˜¯ n_layersï¼Œä¸æ˜¯ num_layers
        n_heads=4,
        d_ff=256
    )
    print("âœ… æˆåŠŸåˆ›å»ºTransformeræ¨¡å‹")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    import torch
    batch_size = 2
    src_seq_len = 10
    tgt_seq_len = 12
    
    src = torch.randint(1, 100, (batch_size, src_seq_len))
    tgt = torch.randint(1, 100, (batch_size, tgt_seq_len))
    
    output = model(src, tgt)
    print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸï¼Œè¾“å‡ºå½¢çŠ¶: {output.shape}")
    
    # æµ‹è¯•ç”Ÿæˆ
    generated = model.generate(src, max_len=15)
    print(f"âœ… åºåˆ—ç”ŸæˆæˆåŠŸï¼Œç”Ÿæˆå½¢çŠ¶: {generated.shape}")
    
    # æµ‹è¯•æ³¨æ„åŠ›æœºåˆ¶
    attention = MultiHeadAttention(d_model=64, n_heads=4)
    query = torch.randn(batch_size, src_seq_len, 64)
    key = torch.randn(batch_size, src_seq_len, 64)
    value = torch.randn(batch_size, src_seq_len, 64)
    
    attn_output, attn_weights = attention(query, key, value)
    print(f"âœ… æ³¨æ„åŠ›æœºåˆ¶æˆåŠŸï¼Œè¾“å‡ºå½¢çŠ¶: {attn_output.shape}")
    
    # æµ‹è¯•ä½ç½®ç¼–ç 
    pe = PositionalEncoding(d_model=64, max_len=100)
    x = torch.randn(batch_size, src_seq_len, 64)
    x_with_pe = pe(x)
    print(f"âœ… ä½ç½®ç¼–ç æˆåŠŸï¼Œè¾“å‡ºå½¢çŠ¶: {x_with_pe.shape}")
    
    print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
    
except Exception as e:
    print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()