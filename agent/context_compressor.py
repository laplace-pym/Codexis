"""
Context Compressor - Structured auto-compression for long context governance.

Implements the AU2 algorithm: 92% trigger threshold + 8-segment structured summary.
When conversation token usage exceeds the configured threshold (default 92%),
the compressor intelligently compresses the conversation history into 8 structured
dimensions, preserving critical context while dramatically reducing token count.

Three-layer memory architecture:
  - Short-term: messages[] (current conversation)
  - Mid-term: compressed summaries (AU2 structured compression)
  - Long-term: persistent project memory (CLAUDE.md / config)
"""

import json
from typing import Optional
from dataclasses import dataclass, field

from llm.base import BaseLLM, Message, LLMResponse
from utils.logger import get_logger


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Default context window sizes per provider/model family
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEFAULT_MAX_CONTEXT_TOKENS = {
    "deepseek": 64_000,
    "openai": 128_000,
    "anthropic": 200_000,
}

# Default fallback
DEFAULT_CONTEXT_WINDOW = 64_000

# Default trigger threshold (92%)
DEFAULT_COMPRESSION_THRESHOLD = 0.92

# Approximate characters per token (for estimation when usage data unavailable)
CHARS_PER_TOKEN = 4

# Number of recent messages to preserve uncompressed
PRESERVE_RECENT_MESSAGES = 4


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# AU2 8-Segment Compression Prompt
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
COMPRESSION_PROMPT = """ä½ æ˜¯ä¸€ä¸ªå¯¹è¯å‹ç¼©ä¸“å®¶ã€‚è¯·æŒ‰ç…§ä»¥ä¸‹ 8 ä¸ªç»“æ„åŒ–ç»´åº¦ï¼Œå¯¹ç»™å®šçš„å¯¹è¯å†å²è¿›è¡Œé«˜ä¿çœŸå‹ç¼©æ‘˜è¦ã€‚
è¦æ±‚ï¼šä¿ç•™æ‰€æœ‰å…³é”®æŠ€æœ¯ç»†èŠ‚ã€ä»£ç è·¯å¾„ã€å†³ç­–ç†ç”±ï¼Œå»é™¤å†—ä½™å¯’æš„å’Œé‡å¤å†…å®¹ã€‚

## 1. èƒŒæ™¯ä¸Šä¸‹æ–‡ (Background Context)
- é¡¹ç›®ç±»å‹å’ŒæŠ€æœ¯æ ˆ
- å½“å‰å·¥ä½œç›®æ ‡å’Œç¯å¢ƒ
- ç”¨æˆ·çš„æ€»ä½“ç›®æ ‡

## 2. å…³é”®å†³ç­– (Key Decisions)
- é‡è¦çš„æŠ€æœ¯é€‰æ‹©å’ŒåŸå› 
- æ¶æ„å†³ç­–å’Œè®¾è®¡è€ƒè™‘
- é—®é¢˜è§£å†³æ–¹æ¡ˆçš„é€‰æ‹©

## 3. å·¥å…·ä½¿ç”¨è®°å½• (Tool Usage Log)
- ä¸»è¦ä½¿ç”¨çš„å·¥å…·ç±»å‹å’Œè°ƒç”¨æƒ…å†µ
- æ–‡ä»¶æ“ä½œå†å²ï¼ˆåˆ›å»º/ä¿®æ”¹/è¯»å–çš„æ–‡ä»¶è·¯å¾„ï¼‰
- å‘½ä»¤æ‰§è¡Œç»“æœæ‘˜è¦

## 4. ç”¨æˆ·æ„å›¾æ¼”è¿› (User Intent Evolution)
- éœ€æ±‚çš„å˜åŒ–è¿‡ç¨‹
- ä¼˜å…ˆçº§è°ƒæ•´
- æ–°å¢åŠŸèƒ½éœ€æ±‚

## 5. æ‰§è¡Œç»“æœæ±‡æ€» (Execution Results)
- æˆåŠŸå®Œæˆçš„ä»»åŠ¡
- ç”Ÿæˆçš„ä»£ç å’Œæ–‡ä»¶æ¸…å•
- éªŒè¯å’Œæµ‹è¯•ç»“æœ

## 6. é”™è¯¯ä¸è§£å†³ (Errors and Solutions)
- é‡åˆ°çš„é—®é¢˜ç±»å‹
- é”™è¯¯è§£å†³æ–¹æ³•
- ç»éªŒæ•™è®­

## 7. æœªè§£å†³é—®é¢˜ (Open Issues)
- å½“å‰å¾…è§£å†³çš„æŒ‘æˆ˜
- å·²çŸ¥ä½†æœªå¤„ç†çš„é—®é¢˜
- éœ€è¦åç»­å…³æ³¨çš„äº‹é¡¹

## 8. åç»­è®¡åˆ’ (Future Plans)
- ä¸‹ä¸€æ­¥è¡ŒåŠ¨è®¡åˆ’
- é•¿æœŸç›®æ ‡å’Œè§„åˆ’
- ç”¨æˆ·è¡¨è¾¾çš„æœŸæœ›

è¯·å°†ä»¥ä¸Šä¿¡æ¯å‹ç¼©åˆ° {max_tokens} ä¸ª Token ä»¥å†…ï¼Œä¿æŒæŠ€æœ¯å‡†ç¡®æ€§å’Œä¸Šä¸‹æ–‡è¿ç»­æ€§ã€‚
ç›´æ¥è¾“å‡ºå‹ç¼©åçš„ç»“æ„åŒ–æ‘˜è¦ï¼Œä¸è¦åŒ…å«ä»»ä½•å‰ç¼€è¯´æ˜ã€‚

---

ä»¥ä¸‹æ˜¯éœ€è¦å‹ç¼©çš„å¯¹è¯å†å²ï¼š

{conversation}"""


@dataclass
class CompressionMetrics:
    """Metrics for a compression operation."""
    original_messages: int = 0
    compressed_messages: int = 0
    estimated_original_tokens: int = 0
    estimated_compressed_tokens: int = 0
    compression_ratio: float = 0.0
    compressions_performed: int = 0

    def to_dict(self) -> dict:
        return {
            "original_messages": self.original_messages,
            "compressed_messages": self.compressed_messages,
            "estimated_original_tokens": self.estimated_original_tokens,
            "estimated_compressed_tokens": self.estimated_compressed_tokens,
            "compression_ratio": self.compression_ratio,
            "compressions_performed": self.compressions_performed,
        }


class ContextCompressor:
    """
    Structured auto-compression for long context governance.

    Monitors token usage during agent execution and triggers AU2 8-segment
    compression when usage exceeds the configurable threshold (default 92%).

    Usage:
        compressor = ContextCompressor(llm=my_llm, provider="deepseek")

        # In the execution loop, before each LLM call:
        messages = compressor.maybe_compress(messages)
    """

    def __init__(
        self,
        llm: BaseLLM,
        provider: Optional[str] = None,
        max_context_tokens: Optional[int] = None,
        threshold: float = DEFAULT_COMPRESSION_THRESHOLD,
        preserve_recent: int = PRESERVE_RECENT_MESSAGES,
    ):
        """
        Args:
            llm: The LLM instance used for generating compression summaries.
            provider: Provider name (deepseek/openai/anthropic) for default window size.
            max_context_tokens: Max context window size in tokens. Auto-detected if None.
            threshold: Trigger threshold as ratio (0-1). Default 0.92 (92%).
            preserve_recent: Number of recent messages to keep uncompressed.
        """
        self.llm = llm
        self.threshold = threshold
        self.preserve_recent = preserve_recent
        self.logger = get_logger()

        # Determine max context window
        if max_context_tokens:
            self.max_context_tokens = max_context_tokens
        elif provider and provider in DEFAULT_MAX_CONTEXT_TOKENS:
            self.max_context_tokens = DEFAULT_MAX_CONTEXT_TOKENS[provider]
        else:
            self.max_context_tokens = DEFAULT_CONTEXT_WINDOW

        self.trigger_tokens = int(self.max_context_tokens * self.threshold)

        # Compression target: aim for ~50% of max after compression
        self.target_tokens = int(self.max_context_tokens * 0.5)

        # Metrics tracking
        self.metrics = CompressionMetrics()

        # Track last known token usage from LLM response
        self._last_usage: Optional[dict] = None

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Public API
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def update_usage(self, usage: Optional[dict]) -> None:
        """Update token usage from the latest LLM response."""
        if usage:
            self._last_usage = usage

    def should_compress(self, messages: list[Message]) -> bool:
        """
        Check if compression should be triggered.

        Uses real token usage from LLM if available, falls back to estimation.
        """
        token_count = self._get_token_count(messages)
        return token_count >= self.trigger_tokens

    def maybe_compress(self, messages: list[Message]) -> list[Message]:
        """
        Compress messages if the threshold is exceeded.

        This is the main entry point â€” call this before each LLM invocation
        in the execution loop.

        Returns:
            The (possibly compressed) message list.
        """
        if not self.should_compress(messages):
            return messages

        self.logger.info(
            f"ğŸ—œï¸ Context compression triggered "
            f"(~{self._get_token_count(messages):,} tokens, "
            f"threshold: {self.trigger_tokens:,})"
        )

        try:
            compressed = self._compress(messages)
            return compressed
        except Exception as e:
            self.logger.warning(f"Compression failed, keeping original messages: {e}")
            return messages

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Internal
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _get_token_count(self, messages: list[Message]) -> int:
        """
        Get the current token count.

        Prefers real usage data from the LLM response (prompt_tokens).
        Falls back to character-based estimation.
        """
        # Try real usage data first
        if self._last_usage:
            prompt_tokens = self._last_usage.get("prompt_tokens")
            if prompt_tokens:
                return int(prompt_tokens)
            # Some providers use total_tokens
            total_tokens = self._last_usage.get("total_tokens")
            if total_tokens:
                return int(total_tokens)

        # Fallback: estimate from message content
        return self._estimate_tokens(messages)

    def _estimate_tokens(self, messages: list[Message]) -> int:
        """Estimate token count from message content length."""
        total_chars = sum(len(msg.content or "") for msg in messages)
        return total_chars // CHARS_PER_TOKEN

    def _compress(self, messages: list[Message]) -> list[Message]:
        """
        Execute AU2 8-segment structured compression.

        Strategy:
        1. Keep system message (index 0) intact
        2. Compress middle messages into a structured summary
        3. Keep the N most recent messages intact for continuity

        Returns:
            New message list: [system, compressed_summary, ...recent_messages]
        """
        if len(messages) <= self.preserve_recent + 2:
            # Too few messages to compress meaningfully
            return messages

        # Split messages
        system_msg = messages[0] if messages[0].role.value == "system" else None
        start_idx = 1 if system_msg else 0

        # Messages to compress (middle section)
        compress_end = len(messages) - self.preserve_recent
        messages_to_compress = messages[start_idx:compress_end]

        # Recent messages to preserve
        recent_messages = messages[compress_end:]

        if not messages_to_compress:
            return messages

        # Build conversation text for compression
        conversation_text = self._format_messages_for_compression(messages_to_compress)

        # Generate compression prompt
        prompt = COMPRESSION_PROMPT.format(
            max_tokens=self.target_tokens,
            conversation=conversation_text,
        )

        # Call LLM to compress
        compression_messages = [
            Message.system("ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¯¹è¯å‹ç¼©åŠ©æ‰‹ï¼Œæ“…é•¿ç»“æ„åŒ–ä¿¡æ¯æå–å’Œæ‘˜è¦ã€‚"),
            Message.user(prompt),
        ]

        response = self.llm.chat_sync(
            messages=compression_messages,
            temperature=0.3,  # Lower temperature for more focused compression
            max_tokens=self.target_tokens,
        )

        if not response.content:
            self.logger.warning("Compression returned empty result, keeping originals")
            return messages

        # Build compressed message list
        compressed = []

        if system_msg:
            compressed.append(system_msg)

        # Insert compressed summary as a system-injected context message
        compressed.append(Message.user(
            f"[Context Compression â€” ä»¥ä¸‹æ˜¯ä¹‹å‰å¯¹è¯çš„ç»“æ„åŒ–æ‘˜è¦]\n\n{response.content}"
        ))
        compressed.append(Message.assistant(
            "ç†è§£ï¼Œæˆ‘å·²æŒæ¡ä¹‹å‰çš„å¯¹è¯ä¸Šä¸‹æ–‡ã€‚è¯·ç»§ç»­ã€‚"
        ))

        # Append recent messages
        compressed.extend(recent_messages)

        # Update metrics
        original_tokens = self._estimate_tokens(messages)
        compressed_tokens = self._estimate_tokens(compressed)

        self.metrics.original_messages = len(messages)
        self.metrics.compressed_messages = len(compressed)
        self.metrics.estimated_original_tokens = original_tokens
        self.metrics.estimated_compressed_tokens = compressed_tokens
        self.metrics.compression_ratio = (
            compressed_tokens / original_tokens if original_tokens > 0 else 1.0
        )
        self.metrics.compressions_performed += 1

        # Reset usage tracking after compression
        self._last_usage = None

        self.logger.info(
            f"âœ… Compression complete: "
            f"{len(messages)} â†’ {len(compressed)} messages, "
            f"~{original_tokens:,} â†’ ~{compressed_tokens:,} tokens "
            f"(ratio: {self.metrics.compression_ratio:.1%})"
        )

        return compressed

    def _format_messages_for_compression(self, messages: list[Message]) -> str:
        """Format messages into a readable text for the compression LLM."""
        parts = []
        for msg in messages:
            role = msg.role.value.upper()
            content = msg.content or ""

            # Truncate very long tool results to save compression input tokens
            if msg.role.value == "tool" and len(content) > 500:
                content = content[:500] + "\n... [truncated]"

            parts.append(f"[{role}]: {content}")

        return "\n\n".join(parts)
